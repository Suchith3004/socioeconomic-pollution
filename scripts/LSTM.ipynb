{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"./sequences.pkl\", \"rb\") as f:\n",
    "    sequences = pkl.load(f)\n",
    "\n",
    "with open(\"./target.pkl\", \"rb\") as f:\n",
    "    target = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim = 30\n",
    "out_dim = 3\n",
    "hidden_dim = 32\n",
    "n_layers = 1\n",
    "batch_size = 32\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "        self.act = nn.Softmax(dim=2)\n",
    "        self.h = (torch.randn(n_layers, batch_size, hidden_dim), \n",
    "                    torch.randn(n_layers, batch_size, hidden_dim))\n",
    "    \n",
    "    def initHiddenCell(self, batch_size=32):\n",
    "        self.h = (torch.randn(n_layers, batch_size, hidden_dim), \n",
    "                    torch.randn(n_layers, batch_size, hidden_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o, self.h = self.lstm(x, self.h)\n",
    "        o = self.fc(o)\n",
    "        o = self.act(o)\n",
    "        pred = torch.argmax(o, dim=2)\n",
    "        return o, pred\n",
    "\n",
    "model = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "train_seq = []\n",
    "train_tar = []\n",
    "for seq, tar in zip(sequences, target):\n",
    "    train_seq.append(torch.tensor(seq))\n",
    "    train_tar.append(torch.tensor(tar))\n",
    "train_seq = pad_sequence(train_seq, batch_first=True, padding_value=0)\n",
    "train_tar = pad_sequence(train_tar, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.shape[0] * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(train_seq, train_tar)\n",
    "train_dataset, test_dataset = random_split(dataset, [261, 65])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosineSimilarity(pred, target):\n",
    "    sim = []\n",
    "    for p, t in zip(pred, target):\n",
    "        p = p.reshape(-1)\n",
    "        sim.append(p.reshape(-1).dot(t).item() / torch.linalg.norm(p) / torch.linalg.norm(t))\n",
    "    return np.mean(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training loss: 0.9319, training accuracy: 610.4000\n",
      "epoch: 1, training loss: 0.9308, training accuracy: 614.1333\n",
      "epoch: 2, training loss: 0.9289, training accuracy: 613.0667\n",
      "epoch: 3, training loss: 0.9340, training accuracy: 610.9333\n",
      "epoch: 4, training loss: 0.9268, training accuracy: 616.2667\n",
      "epoch: 5, training loss: 0.9304, training accuracy: 611.7333\n",
      "epoch: 6, training loss: 0.9310, training accuracy: 611.7333\n",
      "epoch: 7, training loss: 0.9264, training accuracy: 615.4667\n",
      "epoch: 8, training loss: 0.9277, training accuracy: 614.6666\n",
      "epoch: 9, training loss: 0.9310, training accuracy: 612.5333\n",
      "epoch: 10, training loss: 0.9262, training accuracy: 618.6666\n",
      "epoch: 11, training loss: 0.9263, training accuracy: 617.8667\n",
      "epoch: 12, training loss: 0.9275, training accuracy: 617.8666\n",
      "epoch: 13, training loss: 0.9287, training accuracy: 621.3333\n",
      "epoch: 14, training loss: 0.9256, training accuracy: 620.8000\n",
      "epoch: 15, training loss: 0.9264, training accuracy: 617.8667\n",
      "epoch: 16, training loss: 0.9272, training accuracy: 618.6667\n",
      "epoch: 17, training loss: 0.9269, training accuracy: 620.5333\n",
      "epoch: 18, training loss: 0.9250, training accuracy: 618.4000\n",
      "epoch: 19, training loss: 0.9239, training accuracy: 621.8667\n",
      "epoch: 20, training loss: 0.9262, training accuracy: 621.3334\n",
      "epoch: 21, training loss: 0.9244, training accuracy: 622.1334\n",
      "epoch: 22, training loss: 0.9213, training accuracy: 626.1333\n",
      "epoch: 23, training loss: 0.9230, training accuracy: 624.8000\n",
      "epoch: 24, training loss: 0.9291, training accuracy: 620.8000\n",
      "epoch: 25, training loss: 0.9242, training accuracy: 620.8000\n",
      "epoch: 26, training loss: 0.9257, training accuracy: 625.8667\n",
      "epoch: 27, training loss: 0.9256, training accuracy: 627.4667\n",
      "epoch: 28, training loss: 0.9258, training accuracy: 627.7333\n",
      "epoch: 29, training loss: 0.9232, training accuracy: 626.6666\n",
      "epoch: 30, training loss: 0.9260, training accuracy: 620.5333\n",
      "epoch: 31, training loss: 0.9234, training accuracy: 626.9333\n",
      "epoch: 32, training loss: 0.9267, training accuracy: 622.9333\n",
      "epoch: 33, training loss: 0.9255, training accuracy: 628.2667\n",
      "epoch: 34, training loss: 0.9208, training accuracy: 627.2000\n",
      "epoch: 35, training loss: 0.9223, training accuracy: 628.8000\n",
      "epoch: 36, training loss: 0.9218, training accuracy: 628.5333\n",
      "epoch: 37, training loss: 0.9265, training accuracy: 625.6000\n",
      "epoch: 38, training loss: 0.9209, training accuracy: 629.8667\n",
      "epoch: 39, training loss: 0.9225, training accuracy: 626.4000\n",
      "epoch: 40, training loss: 0.9217, training accuracy: 631.2000\n",
      "epoch: 41, training loss: 0.9211, training accuracy: 631.2000\n",
      "epoch: 42, training loss: 0.9215, training accuracy: 626.9333\n",
      "epoch: 43, training loss: 0.9220, training accuracy: 626.9333\n",
      "epoch: 44, training loss: 0.9195, training accuracy: 632.5333\n",
      "epoch: 45, training loss: 0.9244, training accuracy: 623.2000\n",
      "epoch: 46, training loss: 0.9215, training accuracy: 631.2000\n",
      "epoch: 47, training loss: 0.9223, training accuracy: 630.1333\n",
      "epoch: 48, training loss: 0.9236, training accuracy: 626.6667\n",
      "epoch: 49, training loss: 0.9201, training accuracy: 625.6000\n",
      "epoch: 50, training loss: 0.9222, training accuracy: 626.1333\n",
      "epoch: 51, training loss: 0.9172, training accuracy: 633.3333\n",
      "epoch: 52, training loss: 0.9221, training accuracy: 631.2000\n",
      "epoch: 53, training loss: 0.9195, training accuracy: 630.1333\n",
      "epoch: 54, training loss: 0.9230, training accuracy: 632.8000\n",
      "epoch: 55, training loss: 0.9188, training accuracy: 630.6666\n",
      "epoch: 56, training loss: 0.9215, training accuracy: 634.9333\n",
      "epoch: 57, training loss: 0.9214, training accuracy: 633.0667\n",
      "epoch: 58, training loss: 0.9202, training accuracy: 632.2667\n",
      "epoch: 59, training loss: 0.9199, training accuracy: 630.4000\n",
      "epoch: 60, training loss: 0.9173, training accuracy: 634.1334\n",
      "epoch: 61, training loss: 0.9216, training accuracy: 631.2000\n",
      "epoch: 62, training loss: 0.9202, training accuracy: 636.5333\n",
      "epoch: 63, training loss: 0.9188, training accuracy: 632.2667\n",
      "epoch: 64, training loss: 0.9240, training accuracy: 628.5333\n",
      "epoch: 65, training loss: 0.9191, training accuracy: 630.4000\n",
      "epoch: 66, training loss: 0.9185, training accuracy: 634.6667\n",
      "epoch: 67, training loss: 0.9203, training accuracy: 633.8666\n",
      "epoch: 68, training loss: 0.9183, training accuracy: 632.5333\n",
      "epoch: 69, training loss: 0.9173, training accuracy: 633.6000\n",
      "epoch: 70, training loss: 0.9183, training accuracy: 631.2001\n",
      "epoch: 71, training loss: 0.9183, training accuracy: 631.2000\n",
      "epoch: 72, training loss: 0.9221, training accuracy: 624.5333\n",
      "epoch: 73, training loss: 0.9163, training accuracy: 632.8000\n",
      "epoch: 74, training loss: 0.9224, training accuracy: 634.1333\n",
      "epoch: 75, training loss: 0.9156, training accuracy: 637.8667\n",
      "epoch: 76, training loss: 0.9195, training accuracy: 634.6666\n",
      "epoch: 77, training loss: 0.9183, training accuracy: 634.6667\n",
      "epoch: 78, training loss: 0.9139, training accuracy: 630.4000\n",
      "epoch: 79, training loss: 0.9222, training accuracy: 632.0000\n",
      "epoch: 80, training loss: 0.9168, training accuracy: 633.3334\n",
      "epoch: 81, training loss: 0.9187, training accuracy: 635.2000\n",
      "epoch: 82, training loss: 0.9176, training accuracy: 633.8667\n",
      "epoch: 83, training loss: 0.9171, training accuracy: 635.2000\n",
      "epoch: 84, training loss: 0.9154, training accuracy: 639.4667\n",
      "epoch: 85, training loss: 0.9170, training accuracy: 636.2667\n",
      "epoch: 86, training loss: 0.9178, training accuracy: 639.2000\n",
      "epoch: 87, training loss: 0.9198, training accuracy: 633.8667\n",
      "epoch: 88, training loss: 0.9155, training accuracy: 640.2667\n",
      "epoch: 89, training loss: 0.9198, training accuracy: 629.0667\n",
      "epoch: 90, training loss: 0.9159, training accuracy: 628.8000\n",
      "epoch: 91, training loss: 0.9162, training accuracy: 637.6000\n",
      "epoch: 92, training loss: 0.9169, training accuracy: 635.7334\n",
      "epoch: 93, training loss: 0.9187, training accuracy: 637.6000\n",
      "epoch: 94, training loss: 0.9189, training accuracy: 629.8667\n",
      "epoch: 95, training loss: 0.9183, training accuracy: 630.6666\n",
      "epoch: 96, training loss: 0.9205, training accuracy: 634.4000\n",
      "epoch: 97, training loss: 0.9133, training accuracy: 637.0667\n",
      "epoch: 98, training loss: 0.9144, training accuracy: 634.4000\n",
      "epoch: 99, training loss: 0.9148, training accuracy: 636.5333\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    L = []\n",
    "    A = []\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        opt.zero_grad()\n",
    "        model.initHiddenCell()\n",
    "        o, pred = model(x.float())\n",
    "        \n",
    "        loss = criterion(o.reshape(-1, out_dim), y.reshape(-1).long())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        L.append(loss.item())\n",
    "\n",
    "        acc = torch.sum(pred == y) / y.shape[1] * y.shape[0]\n",
    "        A.append(acc)\n",
    "\n",
    "    print(\"epoch: %d, training loss: %.4f, training accuracy: %.4f\" % (epoch, np.mean(L), np.mean(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LSTM.model\", \"wb\") as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = None\n",
    "y_true = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (x, y) in enumerate(test_dataloader):\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        o, pred = model(x.float())\n",
    "        if y_pred is None:\n",
    "            y_pred = pred\n",
    "            y_true = y\n",
    "        else:\n",
    "            torch.cat((y_pred, pred), dim=0)\n",
    "            torch.cat((y_true, y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.94      0.65       242\n",
      "           2       0.12      0.01      0.02       201\n",
      "           3       0.00      0.00      0.00        37\n",
      "\n",
      "    accuracy                           0.48       480\n",
      "   macro avg       0.20      0.32      0.22       480\n",
      "weighted avg       0.30      0.48      0.33       480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = y_pred.reshape(-1).numpy()\n",
    "y_true = y_true.reshape(-1).numpy()\n",
    "target_names = [\"1\", \"2\", \"3\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
